{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikiran37130204/NLP-in-class-activities/blob/main/Activity_3_Normalization_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activity 3: Normalization Solution**\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "\n",
        "\n",
        "* Purpose of this activity is to practice and get hands on experience (Ungraded Activity)  \n",
        "\n",
        "* No Dataset required for this Activity"
      ],
      "metadata": {
        "id": "Ur1Spl3neyQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8hFNQohPORn"
      },
      "source": [
        "# **Text Preprocessing Beyond Tokenization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Word** **normalization**\n",
        "**Word normalization** is a text preprocessing technique used in natural language processing (NLP) to standardize and simplify words or tokens in a text document. The goal of word normalization is to make text data more consistent and manageable for analysis. This process can involve various transformations, such as converting all text to lowercase, removing punctuation, expanding contractions, and performing tasks like stemming or lemmatization to reduce words to their base or dictionary forms. Word normalization helps improve the accuracy and effectiveness of NLP tasks by reducing the complexity of text data and ensuring that similar words are treated as equivalent."
      ],
      "metadata": {
        "id": "HZIYqdWPmHr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peiyYN47rMy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a7bf7b-06c3-49bb-b5d7-37b1c12ef990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# for using NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for using SpaCy\n",
        "import spacy\n",
        "\n",
        "# for HuggingFace\n",
        "!pip install transformers\n",
        "# !pip install ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3Bh5T8sJXFQ"
      },
      "outputs": [],
      "source": [
        "# trick to wrap text to the viewing window for this notebook\n",
        "# Ref: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "#helps improve the readability and formatting of code output in the notebook.\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Revisiting Tokenization** :**TreebankWordTokenizer**\n",
        "\n",
        "The **Treebank Word Tokenizer** is a text processing tool used in natural language processing (NLP) to split text into individual words or tokens. It follows the tokenization conventions and standards of the Penn Treebank corpus"
      ],
      "metadata": {
        "id": "Id3muvWNlNj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text=\"Hello everyone. Welcome to NLP Course.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KiRKo3DKlfuQ",
        "outputId": "355ae131-cfe7-4f43-e3df-353683ac3dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'NLP', 'Course', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Using Regular Expression**\n",
        "\n",
        "**RegexpTokenizer** is a text processing tool provided by the Natural Language Toolkit (NLTK) library in Python. It is used to tokenize (split) text into individual tokens (words or phrases) based on a specified regular expression pattern."
      ],
      "metadata": {
        "id": "43OOaeg7o78C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")#[\\w']+ as a whole matches sequences of word characters (letters, digits, underscores) and single quotes in a string\n",
        "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "me379siro8Jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "f35e0baa-78bb-44a5-b4ce-08e678e08cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\",\n",
              " 'see',\n",
              " 'how',\n",
              " \"it's\",\n",
              " 'working',\n",
              " 'We',\n",
              " 'also',\n",
              " 'have',\n",
              " 'digits',\n",
              " 'like',\n",
              " '123',\n",
              " 'and',\n",
              " '010']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(Titorial) Tokenizing text using Spacy**\n",
        "\n",
        "Following is a dummy sample of text to demonstrate tokenization in SpaCy."
      ],
      "metadata": {
        "id": "2aGjiBnLylj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_text1 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "XeNE7b_xymIL",
        "outputId": "50c5fff1-5a88-4ad9-920b-76447f8cb4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loads a trained English pipeline with specific preprocessing components\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# using SpaCy's tokenizer...\n",
        "doc = nlp(dummy_text1)      # applies the processing pipeline on the text\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UwqTRVHdyoCt",
        "outputId": "ab37610c-55d1-4b38-a51b-82bf39c28b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here\n",
            "is\n",
            "the\n",
            "First\n",
            "Paragraph\n",
            "and\n",
            "this\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "first\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Now\n",
            ",\n",
            "it\n",
            "is\n",
            "the\n",
            "Second\n",
            "Paragraph\n",
            "and\n",
            "its\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "second\n",
            "paragraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Finally\n",
            ",\n",
            "this\n",
            "is\n",
            "the\n",
            "Third\n",
            "Paragraph\n",
            "and\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            "of\n",
            "this\n",
            "paragraph\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "third\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "4th\n",
            "paragraph\n",
            "just\n",
            "has\n",
            "one\n",
            "sentence\n",
            "in\n",
            "it\n",
            ".\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** Copy the above snippet code and Modify the above regular expression to match only **digits** from the given text.  "
      ],
      "metadata": {
        "id": "jaHfuJo-phTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "In the year 2024, advancements in technology continued to revolutionize industries across the globe. Artificial intelligence, machine learning, and data science played critical roles in optimizing processes, predicting outcomes, and driving innovation. Companies like Google, Amazon, and Tesla invested billions in research to stay ahead of the competition.\n",
        "\n",
        "One major breakthrough was the development of quantum computing, which promised to solve complex problems exponentially faster than traditional computers. Researchers at IBM and Microsoft reported significant progress in creating stable qubits, a key component in quantum systems.\n",
        "\n",
        "Meanwhile, in healthcare, personalized medicine reached new heights. Genetic testing became more accessible, allowing doctors to tailor treatments to individual patients. CRISPR technology made headlines with successful gene-editing trials, curing diseases that were previously considered incurable.\n",
        "\n",
        "However, with all these advancements came new challenges. Cybersecurity threats continued to evolve, with ransomware attacks becoming more sophisticated and targeting critical infrastructure. Governments and organizations were forced to bolster their defenses and invest in new cybersecurity technologies to protect sensitive information.\n",
        "\n",
        "As the world faced the effects of climate change, renewable energy sources like solar and wind became increasingly important. Countries around the world set ambitious goals to reduce carbon emissions, with a focus on transitioning to cleaner energy alternatives.\n",
        "\n",
        "By the end of 2025, society had made great strides in many fields, but the rapid pace of change also raised ethical questions. The role of artificial intelligence in decision-making, the implications of gene editing, and the balance between technological progress and privacy remained hotly debated topics as the world prepared for the next wave of innovations.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8VzjtK9ql-17",
        "outputId": "5ff6d3ea-8852-4c45-a3ed-cd548ff59138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "tokenizer = RegexpTokenizer(\"\\d+\")\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "ZiQuSrzQphoo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a8ecc83-a4cf-4404-fb63-dcb79815b751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2024', '2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgaX-Ck7YYzY"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUmlAHIQ9gs"
      },
      "source": [
        "## ** (Tutorial) Stemming and Lemmatization using NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is a text normalization technique in natural language processing and information retrieval. It involves reducing words to their root or base form, often by removing suffixes or prefixes. The goal of stemming is to convert words with the same meaning but different forms into a common base form so that they can be treated as equivalent during text analysis and retrieval. Stemming helps improve information retrieval and text processing tasks by reducing the complexity of words while maintaining their core meaning. Common stemming algorithms include the Porter Stemmer and Snowball Stemmer."
      ],
      "metadata": {
        "id": "Rxa-khLuaEZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Porter Stemmer**\n",
        "The **Porter Stemmer** is a well-known algorithm for stemming in natural language processing. It was  designed to reduce words to their root or base form by removing common suffixes. Stemming is the process of reducing words to their linguistic root or base form to simplify text analysis and improve information retrieval.For example, it can convert words like \"running,\" \"runs,\" and \"ran\" to their common root \"run.\""
      ],
      "metadata": {
        "id": "7TPOS73taHEq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jCebsYwiQxf"
      },
      "source": [
        "Let's see how we can perform stemming and lemmatization using NLTK library..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3M0mDUIiSdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d582e21e-8b69-441d-8371-5f56011bcf40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after stemming: cat\n",
            "'better' after stemming: better\n",
            "'abaci' after stemming: abaci\n",
            "'aardwolves' after stemming: aardwolv\n",
            "'generically' after stemming: gener\n"
          ]
        }
      ],
      "source": [
        "# importing PorterStemmer class from nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
        "\n",
        "stem = porter.stem('cats')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'cats' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('better')\n",
        "print(f\"'better' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('abaci')\n",
        "print(f\"'abaci' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('aardwolves')\n",
        "print(f\"'aardwolves' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('generically')\n",
        "print(f\"'generically' after stemming: {stem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lemmatization**\n",
        "**Lemmatization** is a natural language processing technique that reduces words to their base or dictionary form, known as a \"lemma.\" Unlike stemming, which often involves removing suffixes to approximate a word's root, lemmatization considers the word's context and grammatical meaning. The goal is to transform different inflected forms of a word into a common base form. Lemmatization is particularly useful for maintaining the grammatical correctness of words in text analysis and information retrieval tasks."
      ],
      "metadata": {
        "id": "WOhnG5IIcaHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**WordNet Lemmatizer**\n",
        "The **WordNet Lemmatizer** is a lemmatization tool based on WordNet, a lexical database of the English language. WordNet groups words into sets of synonyms called \"synsets\" and provides a rich lexical and semantic structure for the English language. The WordNet Lemmatizer uses this semantic information to perform lemmatization, which is the process of reducing words to their base or dictionary form (lemma)"
      ],
      "metadata": {
        "id": "tURX72sldJvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "8wVrYUPXS2Oy",
        "outputId": "09305c58-7b6b-416d-fc5b-04904971b416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jo-qgQfjuli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "40b4d9c1-e825-4323-9dfc-34ed319d8588"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after lemmatization: cat\n",
            "'better' after lemmatization: better\n",
            "'abaci' after lemmatization: abacus\n",
            "'aardwolves' after lemmatization: aardwolf\n",
            "'generically' after lemmatization: generically\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'better' (as an adjective) after lemmatization: good\n"
          ]
        }
      ],
      "source": [
        "# importing Word Net-based lemmatizer class from nltk.stem module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()    # instantiating an object of the Word NetLemmatizer class\n",
        "\n",
        "lemma = lemmatizer.lemmatize('cats')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'cats' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('better')\n",
        "print(f\"'better' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('abaci')\n",
        "print(f\"'abaci' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('aardwolves')\n",
        "print(f\"'aardwolves' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('generically')\n",
        "print(f\"'generically' after lemmatization: {lemma}\")\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "lemma = lemmatizer.lemmatize('better', pos='a')   # 'a' denoted ADJECTIVE part-of-speech\n",
        "print(f\"'better' (as an adjective) after lemmatization: {lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGVeoHqrQkF"
      },
      "source": [
        "### **Stemming on text string**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_text(text):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply the stemmer to each word and join them back into a text\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"He is jumping, and he jumped over the jumps.\"\n",
        "stemmed_text = stem_text(text)\n",
        "print(stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0OILkAMtu6iS",
        "outputId": "7143c30b-4122-4878-935c-72719bad4bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he is jump , and he jump over the jump .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCtK0QouYj2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f50f9b1e-beab-4e04-de0c-bafc0c866573"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given text:\n",
            "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n"
          ]
        }
      ],
      "source": [
        "# This is the text on which you have to perform stemming; taken from Wikipedia.\n",
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "print(\"Given text:\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial **"
      ],
      "metadata": {
        "id": "Cl9AIuruwUrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el7w7c7HmY9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "2045eece-1d92-4685-8ef2-9942669bb09b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After punctuation removal:\n",
            "in linguistic morphology and information retrieval stemming is the process of reducing inflected or sometimes derived words to their word stem base or root form generally a written word form the stem need not be identical to the morphological root of the word it is usually sufficient that related words map to the same stem even if this stem is not in itself a valid root\n",
            "\n",
            "\n",
            "After stopword removal:\n",
            "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'words', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'words', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ],
      "source": [
        "#CODE BLOCK 1\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "def remove_punc(text_string):\n",
        "  return re.sub('[^a-zA-Z0-9 ]', '', text_string.lower())\n",
        "\n",
        "def remove_stopwords(text_string):\n",
        "  return [ token for token in text_string.split(' ') if token not in en_stopwords ]\n",
        "\n",
        "# applying punctuation removal to the text\n",
        "unpunc_text = remove_punc(text)\n",
        "print(\"After punctuation removal:\")\n",
        "print(unpunc_text)\n",
        "\n",
        "# # applying stopword removal to the text\n",
        "clean_text = remove_stopwords(unpunc_text)\n",
        "print(\"\\n\\nAfter stopword removal:\")\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5uju7eGVfg"
      },
      "source": [
        "#### **Question 2. Perform stemming on the given cleaned text using the Porter Stemmer from NLTK.**\n",
        "\n",
        "**Hint:** import PorterStemmer from nltk.stem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "words = word_tokenize(text_no_punct.lower())\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "NQLwD-N4nIUm",
        "outputId": "3532f32b-83f4-4728-dfe2-ffbd6a066cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['linguist', 'morpholog', 'inform', 'retriev', 'stem', 'process', 'reduc', 'inflect', 'sometim', 'deriv', 'word', 'word', 'stem', 'base', 'root', 'form', 'gener', 'written', 'word', 'form', 'stem', 'need', 'ident', 'morpholog', 'root', 'word', 'usual', 'suffici', 'relat', 'word', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization on text string**"
      ],
      "metadata": {
        "id": "4bUoRnNN6t0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ib7DcVk3r9Aj",
        "outputId": "057c0bd2-0884-4a0e-c7fc-154c654a027d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example text to lemmatize\n",
        "text = \"There are like more than 100 foxes and lions in this forest.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = text.split()\n",
        "\n",
        "# Lemmatize each word and join them back into a sentence\n",
        "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fp2nzJba6sJZ",
        "outputId": "874441b9-89b5-4745-b7ce-b20ceea569f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: There are like more than 100 foxes and lions in this forest.\n",
            "Lemmatized Text: There are like more than 100 fox and lion in this forest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N44TNDasS6eK"
      },
      "source": [
        "#### **Question 3. Perform lemmatization on the given cleaned text above using NLTK's lemmatizer.**\n",
        "\n",
        "**Hint**:import WordNetLemmatizer from nltk.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ-moA25Erh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e380f5a8-ca05-4823-ea6e-0eb6d85e2120"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'word', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'word', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ],
      "source": [
        "# apply NLTK's lemmatizer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "#CODE HERE\n",
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "words = word_tokenize(text_no_punct.lower())\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNe9XfmOcqi"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**(Tutorial)** **Sentence Segmentation using Spacy**\n",
        "\n",
        "Following is a dummy paragraph of text to demonstrate how to use SpaCy to segment text into sentences."
      ],
      "metadata": {
        "id": "hozuGxNkz4PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_text3 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "vvUftEvFz5QM",
        "outputId": "69d99722-57e4-44b8-eda4-4107dab72610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# performing sentence splitting...\n",
        "doc = nlp(dummy_text3)\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "p1JkWOh2z9ET",
        "outputId": "ea827c5b-0147-40c6-870e-593ff31590e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the first paragaraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "Now, it is the Second Paragraph and its First Sentence.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the second paragraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph.\n",
            "Here is the Second Sentence.\n",
            "Now is the Third Sentence.\n",
            "This is the Fourth Sentence of the third paragaraph.\n",
            "This paragraph is ending now with a Fifth Sentence.\n",
            "\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYE8A-Mk4BKe"
      },
      "source": [
        "## **(Tutorial) Subword Tokenization using HuggingFace**\n",
        "\n",
        "**Hugging Face** is used for subword tokenization by offering NLP practitioners access to pre-trained subword tokenizers and models. Hugging Face's \"transformers\" library offers pre-trained models and tokenizers, such as Byte Pair Encoding (BPE) and SentencePiece, which are widely used for subword tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subword tokenization** is a text processing technique used in natural language processing (NLP) to break down words into smaller units, often subword pieces. This approach is particularly useful for handling languages with complex morphology or when dealing with out-of-vocabulary words. Subword tokenization methods like Byte-Pair Encoding (BPE) and SentencePiece divide text into subword units, such as character-level tokens or subword pieces, allowing NLP models to work with a more extensive and adaptable vocabulary. This technique improves the handling of rare words and enhances the performance of NLP models on a wide range of languages and tasks"
      ],
      "metadata": {
        "id": "Y48GHrsfgHI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV2KUGEhIm2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "01f4ced9-4b49-470f-90bb-5d057d85f4ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
            "--2024-09-09 16:20:52--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.96.248, 52.217.227.80, 52.216.36.128, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.96.248|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-medium-vocab.json’\n",
            "\n",
            "gpt2-medium-vocab.j 100%[===================>]   1018K   925KB/s    in 1.1s    \n",
            "\n",
            "2024-09-09 16:20:54 (925 KB/s) - ‘gpt2-medium-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2024-09-09 16:20:54--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.96.248, 52.217.227.80, 52.216.36.128, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.96.248|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K   607KB/s    in 0.7s    \n",
            "\n",
            "2024-09-09 16:20:56 (607 KB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "#This is a JSON file that contains the vocabulary (i.e., the set of words and subword pieces) used by the GPT-2 model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte Pair Encoding (BPE)** is a subword tokenization technique used in natural language processing (NLP) and text processing. It involves dividing text into subword units, typically based on frequency, to create a more flexible and adaptive vocabulary for language models."
      ],
      "metadata": {
        "id": "0wdDDf6VgnwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyiWXf-hLtRF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fb6e4916-168c-4a69-bcf2-3137912173d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
            "['The', 'Ġcustom', 'Ġof', 'Ġdelivering', 'Ġan', 'Ġaddress', 'Ġon', 'ĠIn', 'aug', 'uration', 'ĠDay', 'Ġstarted', 'Ġwith', 'Ġthe', 'Ġvery', 'Ġfirst', 'ĠIn', 'aug', 'uration', 'âĢĶ', 'George', 'ĠWashington', 'âĢ', 'Ļ', 's', 'âĢĶ', 'on', 'ĠApril', 'Ġ30', ',', 'Ġ17', '89', '.']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
        "gpt2merges = \"gpt2-merges.txt\"\n",
        "\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "bpe_encoding = bpe.encode(\"The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\")\n",
        "print(bpe_encoding.ids)\n",
        "print(bpe_encoding.tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: Collect the encoding ids which you generate from above snippet and now decode the ids to get back the given text string?\n",
        "\n",
        "**Hint**: use decode() method"
      ],
      "metadata": {
        "id": "lvqxvTMqj5qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
        "gpt2merges = \"gpt2-merges.txt\"\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "text = \"The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\"\n",
        "bpe_encoding = bpe.encode(text)\n",
        "print(\"Encoded IDs:\", bpe_encoding.ids)\n",
        "print(\"Tokens:\", bpe_encoding.tokens)\n",
        "decoded_text = bpe.decode(bpe_encoding.ids)\n",
        "print(\"Decoded Text:\", decoded_text)\n"
      ],
      "metadata": {
        "id": "GaRsld66j5Ih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "457164f5-297b-488a-c14c-7f0139527d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded IDs: [464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
            "Tokens: ['The', 'Ġcustom', 'Ġof', 'Ġdelivering', 'Ġan', 'Ġaddress', 'Ġon', 'ĠIn', 'aug', 'uration', 'ĠDay', 'Ġstarted', 'Ġwith', 'Ġthe', 'Ġvery', 'Ġfirst', 'ĠIn', 'aug', 'uration', 'âĢĶ', 'George', 'ĠWashington', 'âĢ', 'Ļ', 's', 'âĢĶ', 'on', 'ĠApril', 'Ġ30', ',', 'Ġ17', '89', '.']\n",
            "Decoded Text: The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\n"
          ]
        }
      ]
    }
  ]
}