{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia6KKm8R8mYY"
   },
   "source": [
    "##Instructions:\n",
    "##1.Submit a ipynb file and pdf file\n",
    "##2.This assignment is for practice purposes and won't be graded or scored.\n",
    "##3.This assignment is specially for newcomers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4OkY3AFCIdB"
   },
   "source": [
    "##Exercise -1\n",
    "Here is example for print function,Now using the print() funtion create variable and print your name and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYti22WW8a0E",
    "outputId": "9bd677d8-6d9b-4954-c72f-5756ff47cbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "##example\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYJiEVzF8cRv"
   },
   "outputs": [],
   "source": [
    "##your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsZ-2fTNMbm9"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjoAPXdxgOQr"
   },
   "source": [
    "##Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units called tokens. It is a fundamental step in natural language processing (NLP) tasks. Tokenization breaks down sentences and paragraphs into individual words, punctuation, and numbers - capturing the basic linguistic units for analysis. Effective tokenization is key for feeding text data into machine learning models and deriving insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hfCv2moMdOy",
    "outputId": "d7b57af6-763a-405a-a78a-34652c19c41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSwJx_xKO0g9",
    "outputId": "c0d3f6ed-898e-424f-8e8a-c775f15afd69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Np4ov3eZSSaW",
    "outputId": "9e781496-d3fb-40ce-8d09-dd35e9b98bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L21tgTIFO38u",
    "outputId": "9392ec75-098f-42d5-86ee-ca4025370214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(\"tokens:\",tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDLxf-sCyvbG"
   },
   "source": [
    "##Exercise -2\n",
    "##Using the above methods implement tokenization on your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xHKbGBYPKVb"
   },
   "outputs": [],
   "source": [
    "##your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BY_I65bRpl3"
   },
   "source": [
    "##re.split()\n",
    "##re.split() is useful for tokenizing strings in natural language processing and provides an alternative to string.split() with added regex power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjUGqFKA-wvs",
    "outputId": "2a7feca9-7a2e-40b6-f7e4-928b1ee3a76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'for', 'regex', 'tokenization', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is a sample sentence for regex tokenization.\"\n",
    "\n",
    "tokens = re.split(\"\\W+\", text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtrn65alRHbR"
   },
   "source": [
    "##Exercise -3\n",
    "## Implement  re.split funtion to your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHnTnuw4RE_Z"
   },
   "outputs": [],
   "source": [
    "##your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNTEsCbTR9hF"
   },
   "source": [
    "##Exercise -4\n",
    "##Using your own dataset display the head of the data and tail of the data and also plot a simple line graph\n",
    "(You can use any dataset which is available online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doxT4n25rS53"
   },
   "outputs": [],
   "source": [
    "##your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFBCBrzyN9qf"
   },
   "source": [
    "##References\n",
    "##1.https://developers.google.com/edu/python/exercises/basic\n",
    "##2.https://www.w3schools.com/python/\n",
    "##3.https://docs.python.org/3/tutorial/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
