{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saikiran37130204/NLP-in-class-activities/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ASSIGNMENT 1**\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Answer the questions in brief with examples.\n",
        "\n",
        "* Submit  \n",
        "1. The IPython Notebook (ipynb) file.  \n",
        "2. A PDF version of the notebook (converted from ipynb).\n",
        "3. The similarity score should be less than 15%"
      ],
      "metadata": {
        "id": "wknRrNHwjodK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 1: Tokenization & & Sentence Segmentation(20%)**\n",
        "(refer to the spacy tokenization concept which is explained after Question1 in activity-1)\n",
        "\n",
        "**Question -1:**\n",
        "\n",
        "##How can incorporating contextual information beyond single words enhance tokenization and improve performance in downstream tasks such as machine translation or question answering?"
      ],
      "metadata": {
        "id": "-0MGUnuLnXue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will significantly improves the performance in some traditional tasks like machine translation or question answering by solving problems related to word-level tokenization that by handling polysemy(words having multiple meaning), long-range dependencies(helps in understanding relationship between question and answer), sub-word representation and idiomatic expressions which is important in achieving the high-quality results."
      ],
      "metadata": {
        "id": "T9jooJ_D1UQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Question 2**\n",
        "##Develop a tokenizer that considers contractions like \"can't\" and \"doesn't\", splitting them into their constituent words."
      ],
      "metadata": {
        "id": "VIl7PgRaeGjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text=\"I can't believe it's not butter! You're going to love it.\"\n",
        "\n",
        "# Dictionary for common contractions\n",
        "contractions_dict = {\n",
        "    \"can't\": [\"can\", \"not\"],\n",
        "    \"won't\": [\"will\", \"not\"],\n",
        "    \"shan't\": [\"shall\", \"not\"],\n",
        "    \"n't\": [\" not\"],\n",
        "    \"'re\": [\" are\"],\n",
        "    \"'ll\": [\" will\"],\n",
        "    \"'ve\": [\" have\"],\n",
        "    \"'d\": [\" would\"],\n",
        "    \"'m\": [\" am\"],\n",
        "    \"'s\": [\" is\"]\n",
        "}\n",
        "\n",
        "# Regular expressions to detect common contractions\n",
        "contraction_re = re.compile(r\"\\b(can't|won't|shan't|n't|'re|'ll|'ve|'d|'m|'s)\\b\", re.IGNORECASE)\n",
        "\n",
        "def expand_contraction(match):\n",
        "    contraction = match.group(0).lower()  # Get matched contraction\n",
        "    return ' '.join(contractions_dict[contraction])\n",
        "\n",
        "def tokenize_with_contractions(text):\n",
        "    # First expand contractions using regular expressions\n",
        "    expanded_text = contraction_re.sub(expand_contraction, text)\n",
        "\n",
        "    # Tokenize the text by splitting on spaces\n",
        "    tokens = word_tokenize(expanded_text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test the tokenizer\n",
        "tokens = tokenize_with_contractions(text)\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "aQ_rfINQfbA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7070da7-22e9-4a6d-a646-2274ac1709c4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'can', 'not', 'believe', 'it', 'is', 'not', 'butter', '!', 'You', 'are', 'going', 'to', 'love', 'it', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 3:**\n",
        "##Implement a Python script to remove Twitter username handles from a given twitter text.\n",
        "\n"
      ],
      "metadata": {
        "id": "1BRekASOnmsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#CODE HERE\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_twitter_handles(text):\n",
        "    # Regular expression pattern to match Twitter handles\n",
        "    handle_pattern = r'@\\w+'\n",
        "\n",
        "    # Use re.sub to replace handles with an empty string\n",
        "    cleaned_text = re.sub(handle_pattern, '', text)\n",
        "\n",
        "    # Strip extra spaces that might be left after removing handles\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Example usage\n",
        "text=\"Great meeting with @JohnDoe and @JaneSmith today! Looking forward to our next project. Thanks for the insights @TechGuru üöÄ #innovation #teamwork\"\n",
        "cleaned_text = remove_twitter_handles(text)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "-MD-tQZKsCkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e17f76-01b7-43b3-d6df-4450b1866004"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great meeting with and today! Looking forward to our next project. Thanks for the insights üöÄ #innovation #teamwork\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:**\n",
        "\n",
        "##In many NLP tasks, especially tasks involving large documents, it's important to not just tokenize words but also segment sentences. Write a Python script to segment a multi-paragraph text into individual sentences, considering the following cases:\n",
        "\n",
        "Sentences ending with abbreviations like \"Dr.\", \"Mr.\", etc.\n",
        "Sentences that contain dialogue within quotation marks.\n",
        "Use spaCy or NLTK for sentence segmentation and briefly explain how contextual awareness impacts this process."
      ],
      "metadata": {
        "id": "PyRZ4X3Aucy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input_example_text = 'Dr. Smith said, \"Let\\'s meet at 5 p.m. at the cafe.\" She smiled and walked away. The meeting went well.'"
      ],
      "metadata": {
        "id": "fv3DDOgFuhbe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code here\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def segment_sentences(text):\n",
        "    # Process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract sentences from the spaCy document object\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "# Segment the text into sentences\n",
        "segmented_sentences = segment_sentences(Input_example_text)\n",
        "\n",
        "# Print the segmented sentences\n",
        "for i, sentence in enumerate(segmented_sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n"
      ],
      "metadata": {
        "id": "87IxZLTXukA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14a6022-e8c1-45bf-e9be-be3cf78f909f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Dr. Smith said, \"Let's meet at 5 p.m. at the cafe.\"\n",
            "Sentence 2: She smiled and walked away.\n",
            "Sentence 3: The meeting went well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 2.  - Regular Expressions (30%)**"
      ],
      "metadata": {
        "id": "zME8pbf7KWZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Regular Expressions**\n",
        "A regular expression, often abbreviated as regex, is a powerful and flexible tool for pattern matching and text manipulation. It consists of a sequence of characters that defines a search pattern, allowing you to perform various text-related tasks such as text validation, data extraction, text cleaning, and more. Regular expressions are used in programming languages and text editors and are constructed using a combination of regular characters, special characters, and metacharacters to specify search criteria. Learning to use regular expressions effectively can greatly enhance text processing tasks, making them a valuable skill in fields like natural language processing, data extraction, and data validation."
      ],
      "metadata": {
        "id": "tRR6kMx1haKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python includes a builtin module called `re` which provides regular expression matching operations (Click [here](https://docs.python.org/3/library/re.html) for the official module documentation). Once the module is imported into your code, you can use all of the available capabilities for performing pattern-based matching or searching using regular expressions."
      ],
      "metadata": {
        "id": "hhp7XiXXh-CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##**Question - 1**\n",
        "\n",
        "##Write a Python script that uses regular expressions to extract all email addresses and URLs from a text. Ensure your script handles:\n",
        "\n",
        "##Multiple TLDs (e.g., .com, .org)\n",
        "##Edge cases like emails with numbers (e.g., john123@domain.com)"
      ],
      "metadata": {
        "id": "ojyfJF7bymRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input_text = \"For more details, contact us at support@myemail.com or visit https://example.org. Alternatively, you can email sales@company123.org.\""
      ],
      "metadata": {
        "id": "evyfHbBrveNw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "import re\n",
        "\n",
        "def extract_emails_and_urls(text):\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    url_pattern = r'(https?://[^\\s]+|www\\.[^\\s]+)'\n",
        "\n",
        "    emails = re.findall(email_pattern, text)\n",
        "\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    return emails, urls\n",
        "\n",
        "emails, urls = extract_emails_and_urls(Input_text)\n",
        "\n",
        "print(\"Emails found:\", emails)\n",
        "print(\"URLs found:\", urls)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "wlcFDE341Idg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49794f8-397c-40a5-939e-5825969d3276"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails found: ['support@myemail.com', 'sales@company123.org']\n",
            "URLs found: ['https://example.org.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Question- 2**\n",
        "\n",
        "##Implement a Python program to find URLs in the given string using Regular expression."
      ],
      "metadata": {
        "id": "17YpjyzUo8oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
        "##Your code here\n",
        "import re\n",
        "\n",
        "def extract_urls(text):\n",
        "    url_pattern = r'href=\"(https?://[^\\s]+|www\\.[^\\s]+)\"'\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    return urls\n",
        "\n",
        "urls = extract_urls(text)\n",
        "\n",
        "print(\"URLs found:\", urls)"
      ],
      "metadata": {
        "id": "gRsGKmmskWJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eac37e1-c416-4889-d285-e7685d36b521"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URLs found: ['https://w3resource.com', 'http://github.com', 'https://openai.com', 'https://docs.python.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using regular expressions based pattern matching on real world text**\n",
        "\n",
        "For the purposes of demonstration, here's a dummy paragraph of text. A few observations here:\n",
        "* The text has multiple paragraphs with each paragraph having more than one sentence.\n",
        "* Some of the words are capitalized (first letter is in uppercase followed by lowercase letters)."
      ],
      "metadata": {
        "id": "Uf1Pm9CHs1LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph is not going to be detected by either of the regex patterns below.\n",
        "\"\"\"\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPLUtLxVtiTH",
        "outputId": "9a73b04e-5361-4347-c2b1-d56e23590670"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph is not going to be detected by either of the regex patterns below.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code block shows a regular expression that matches only those strings that:\n",
        "1. are at the start of a line and\n",
        "2. the string does not start with a number or a whitespace\n",
        "\n",
        "`re.findall()` finds all matches of the pattern in the text under consideration. The output is a list of strings that matched."
      ],
      "metadata": {
        "id": "zqmf8KSFtt2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further, the regular expression defined below matches the words that are capitalized."
      ],
      "metadata": {
        "id": "UyYWQ5Tzttzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##code block - 3\n",
        "re_pattern2 = r'[A-Z][a-z]+'\n",
        "print(re.findall(re_pattern2, text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxA3QOHHtqOg",
        "outputId": "26f18257-6350-45c4-a802-9689d69ef5f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here', 'First', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence', 'Now', 'Second', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence', 'Finally', 'Third', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Following is a text excerpt on \"Inaugural Address\" taken from the website of the [Joint Congressional Committee on Inaugural Ceremonies](https://www.inaugural.senate.gov/inaugural-address/):"
      ],
      "metadata": {
        "id": "J0VUlW3RunCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inau_text=\"\"\"The custom of delivering an address on Inauguration Day started with the very first Inauguration‚ÄîGeorge Washington‚Äôs‚Äîon April 30, 1789(04-30-1789). ex:-18.5. After taking his oath of office on the balcony of Federal Hall in New York City, Washington proceeded to the Senate chamber where he read a speech before members of Congress and other dignitaries. His second Inauguration took place in Philadelphia on March 4, 1793(03/04/1793), in the Senate chamber of Congress Hall. There, Washington gave the shortest Inaugural address on record‚Äîjust 135 words ‚Äîbefore repeating the oath of office.\n",
        "Every President since Washington has delivered an Inaugural address. While many of the early Presidents read their addresses before taking the oath, current custom dictates that the Chief Justice of the Supreme Court administer the oath first, followed by the President‚Äôs speech.\n",
        "William Henry Harrison delivered the longest Inaugural address, at 8,445 words, on March 4, 1841‚Äîa bitterly cold, wet day. He died one month later of pneumonia, believed to have been brought on by prolonged exposure to the elements on his Inauguration Day. John Adams‚Äô Inaugural address, which totaled 2,308 words, contained the longest sentence, at 737 words. After Washington‚Äôs second Inaugural address, the next shortest was Franklin D. Roosevelt‚Äôs fourth address on January 20, 1945(01-20-1945), at just 559.0 words. Roosevelt had chosen to have a simple Inauguration at the White House in light of the nation‚Äôs involvement in World War II.\n",
        "In 1921, Warren G. Harding became the first President to take his oath and deliver his Inaugural address through loud speakers. In 1925, Calvin Coolidge‚Äôs Inaugural address was the first to be broadcast nationally by radio. And in 1949, Harry S. Truman became the first President to deliver his Inaugural address over television airwaves.\n",
        "Most Presidents use their Inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today. In 1865, in the waning days of the Civil War, Abraham Lincoln stated, ‚ÄúWith malice toward none, with charity for all, with firmness in the right as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation‚Äôs wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.‚Äù In 1933, Franklin D. Roosevelt avowed, ‚Äúwe have nothing to fear but fear itself.‚Äù And in 1961, John F. Kennedy declared, ‚ÄúAnd so my fellow Americans: ask not what your country can do for you‚Äîask what you can do for your country.‚Äù\n",
        "Today, Presidents deliver their Inaugural address on the West Front of the Capitol, but this has not always been the case. Until Andrew Jackson‚Äôs first Inauguration in 1829, most Presidents spoke in either the House or Senate chambers. Jackson became the first President to take his oath of office and deliver his address on the East Front Portico of the U.S. Capitol in 1829. With few exceptions, the next 37.0 Inaugurations took place there, until 1981, when Ronald Reagan‚Äôs Swearing-In Ceremony and Inaugural address occurred on the West Front Terrace of the Capitol. The West Front has been used ever since. You should also need to extract the floating numbers such as -55.5, 20.8%, -3.0 using your regular expression\"\"\"\n"
      ],
      "metadata": {
        "id": "wRxR7M-NuFtR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to  above code block -3 for the following questions"
      ],
      "metadata": {
        "id": "0qMOPAWe3wnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Questions-3.A**\n",
        "## Identify all the positive and neagtive numbers with type of both intergers,float  in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of such words in the text. Then, run the Python code snippet to automatically display the matched strings according to the pattern.*.\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ],
      "metadata": {
        "id": "_Oap3GNEut1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Your code here\n",
        "re_pattern3=r'[-+]?\\d+\\.?\\d+';\n",
        "print(re.findall(re_pattern3,inau_text))"
      ],
      "metadata": {
        "id": "BNqyUNbPurJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3465d22-72fe-4d4a-b12a-304d6e559d10"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['30', '1789', '04', '-30', '-1789', '-18.5', '1793', '03', '04', '1793', '135', '445', '1841', '308', '737', '20', '1945', '01', '-20', '-1945', '559.0', '1921', '1925', '1949', '1865', '1933', '1961', '1829', '1829', '37.0', '1981', '-55.5', '20.8', '-3.0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The re_pattern3 matches all the positive, negative numbers with integer and float types where '[-+]?' this expression checks for positive and negaitve numbers where they can be optional as we check only for negative integers and float types, '\\d+' checks for one or more digit occurances, '\\.?' checks for point where its an option to check float values(eg 1.1) and finally '\\d+' checks for one or more digits after the float point."
      ],
      "metadata": {
        "id": "IWQdvlG0dPKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question-3.B**\n",
        "##*Identify all the dates of all forms - text form(April 20, 1945) and digit form(xx-xx-xxxx, xx/xx/xxxx) in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of the dates in the text. Then, run the Python code snippet to automatically display a list of all such dates identified.*\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ],
      "metadata": {
        "id": "-mkbLF4nu7la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Your code here\n",
        "re_pattern4=r'(?:(?:[A-Z][a-z]+ \\d{1,2}, \\d{4})|(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{4}))'\n",
        "print(re.findall(re_pattern4,inau_text))"
      ],
      "metadata": {
        "id": "2pxl72VhvEcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac73e1b-34dc-4212-e274-8275ba5a393b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['April 30, 1789', '04-30-1789', 'March 4, 1793', '03/04/1793', 'March 4, 1841', 'January 20, 1945', '01-20-1945']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "re_pattern4 matches all date formats in the form of 'may 30, 2001' or '02-24-1999' or '02/24/1999'"
      ],
      "metadata": {
        "id": "TdUT4vfidQ_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 3: Lemmatization/Stemming(25%)**\n",
        "\n",
        "\n",
        "#**Question -1:**\n",
        "##How does the morphology of a language (e.g., agglutinative vs. fusional) impact the suitability of stemming vs. lemmatization?"
      ],
      "metadata": {
        "id": "QZSO1IrN06xI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here"
      ],
      "metadata": {
        "id": "80ycUYp41Iy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        " ## **Question - 2 :**\n",
        "\n",
        "\n",
        "## Create a Python function that takes a sentence as input, performs lemmatization using **StanfordNLP**, and removes stopwords from the lemmatized sentence. Use a list of stopwords. Return the cleaned and lemmatized sentence.\n",
        "\n",
        "Reference:https://stanfordnlp.github.io/stanfordnlp/\n",
        "\n"
      ],
      "metadata": {
        "id": "EMMf51DdsaK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n"
      ],
      "metadata": {
        "id": "udt8qI_viOXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here"
      ],
      "metadata": {
        "id": "Ri0OIiWfBN8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question - 3**\n",
        "\n",
        "(refer to the byte pair encoding concept which is explained in activity-2 at the Tutorial of Subword Tokenization using HuggingFace after the Question6 in activity-2 )\n",
        "\n",
        "\n",
        "Consider the following two sentences:\n",
        "\n",
        "**S1:**I like yellow roses better than red ones.\n",
        "\n",
        "**S2:**Looks like John is bettering the working conditions at his organization\n",
        "\n",
        "**Create a Python function that encodes two sentences using the custom BPE tokenizer and identifies common subword tokens (tokens that appear in both encodings). Return a list of these common subword tokens. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison?**"
      ],
      "metadata": {
        "id": "I7o1E0wTupIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code here"
      ],
      "metadata": {
        "id": "3RY65-RPvURG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task - 4 : Minimum Edit distance (25%)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Minimum edit Distance\n",
        "Minimum Edit Distance (also known as Levenshtein Distance) is a measure of similarity between two strings by calculating the minimum number of single-character edits (insertions, deletions, substitutions) required to transform one string into the other. It has applications in various fields, including natural language processing, spell checking, DNA sequence alignment, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEKqJAYbvUG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character Based Text Similarity\n",
        "\"As an example, this technology is used by information retrieval systems, search engines, automatic indexing systems, text summarizers, categorization systems, plagiarism checkers, speech recognition, rating systems, DNA analysis, and profiling algorithms (IR/AI programs to automatically link data between people and what they do).\""
      ],
      "metadata": {
        "id": "aNZzSXu2xoAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##code block -4\n",
        "# A Naive recursive Python program to find minimum number\n",
        "# operations to convert str1 to str2\n",
        "\n",
        "\n",
        "def editDistance(str1, str2, m, n):\n",
        "\n",
        "    # If first string is empty, the only option is to\n",
        "    # insert all characters of second string into first\n",
        "    if m == 0:\n",
        "        return n\n",
        "\n",
        "    # If second string is empty, the only option is to\n",
        "    # remove all characters of first string\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    # If last characters of two strings are same, nothing\n",
        "    # much to do. Ignore last characters and get count for\n",
        "    # remaining strings.\n",
        "    if str1[m-1] == str2[n-1]:\n",
        "        return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "    # If last characters are not same, consider all three\n",
        "    # operations on last character of first string, recursively\n",
        "    # compute minimum cost for all three operations and take\n",
        "    # minimum of three values.\n",
        "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert\n",
        "                   editDistance(str1, str2, m-1, n),    # Remove\n",
        "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
        "                   )\n",
        "\n",
        "\n",
        "# Driver code\n",
        "str1 = \"sunday\"\n",
        "str2 = \"saturday\"\n",
        "print (editDistance(str1, str2, len(str1), len(str2)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHA_ccV_xmCB",
        "outputId": "0f8944cb-413d-4740-dd8e-5ba112fbb6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer above code block -4  for the following question"
      ],
      "metadata": {
        "id": "nSfpOff4Ngbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-1\n",
        "##Assuming case sensitivity where changing a letter's case has a cost of 1, calculate the minimum cost to transform \"Imagine\" into \"imagination\" with the following operation costs: insertions = 2, deletions = 2, substitutions = 3"
      ],
      "metadata": {
        "id": "OUJ1n35-dgON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##your code here"
      ],
      "metadata": {
        "id": "bt4ZjDL2c3LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation"
      ],
      "metadata": {
        "id": "6fdCkNgZdL7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial -2\n",
        "# Levenshtein Distance for Sentences"
      ],
      "metadata": {
        "id": "RFiLMn7rIHUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code block - 5\n",
        "# Simple Minimum Edit Distance\n",
        "def levenshtein_distance(str1, str2):\n",
        "    # Initialize a matrix to store edit distances\n",
        "    m, n = len(str1), len(str2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Fill in the matrix using dynamic programming\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 2  # Substitution cost is 2\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,  # Deletion\n",
        "                dp[i][j - 1] + 1,  # Insertion\n",
        "                dp[i - 1][j - 1] + cost,  # Substitution\n",
        "            )\n",
        "\n",
        "    # The final value in the matrix represents the Levenshtein distance\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "str1 = \"This is a cat\"\n",
        "str2 = \"That is a dog\"\n",
        "distance = levenshtein_distance(str1, str2)\n",
        "print(f\"The Levenshtein distance between '{str1}' and '{str2}' with substitution cost 2 is {distance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FypLc-dkO96G",
        "outputId": "2a43ae34-99b2-4de4-e2d6-2bae18be35d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Levenshtein distance between 'This is a cat' and 'That is a dog' with substitution cost 2 is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to above code block -5 from tutorial - 2 for the following question"
      ],
      "metadata": {
        "id": "e-xrzKVC5iSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question:2\n",
        "##Assign different costs to insertions, deletions, and substitutions to reflect varying penalties for different types of edits. Calculate the Levenshtein distance with these weighted costs.\n",
        "\n",
        "String1 = (\"Natural language processing\")\n",
        "\n",
        "String2 = (\"Computer science department\")\n",
        "\n",
        "Provide your explanation in the tex block below"
      ],
      "metadata": {
        "id": "EapfLAq5iCLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ENTER YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "kcDvlYTD4YvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation"
      ],
      "metadata": {
        "id": "texg77AFdG6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question -3\n",
        "\n",
        "##Modify the minimum edit distance algorithm to account for different penalties:\n",
        "\n",
        "Insertion = 1\n",
        "\n",
        "Deletion = 2\n",
        "\n",
        "Substitution = 3\n",
        "\n",
        "Apply it to the following sentences:\n",
        "\n",
        "**Senetence 1 = \"Data Science is evolving fast.**\"\n",
        "\n",
        "**Senetence 2 = \"Artificial Intelligence is transforming industries.**\"\n",
        "\n",
        "\n",
        "Explain how different weights reflect real-world penalties in tasks like document comparison or DNA sequence matching."
      ],
      "metadata": {
        "id": "pg7XS-bAztur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code here"
      ],
      "metadata": {
        "id": "xjRSiKhj0avx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-4\n",
        "##Explain how Minimum Edit Distance (MED) is applied in NLP tasks like spell checking, speech recognition, text summarization, and machine translation. How does its effectiveness differ across these tasks?"
      ],
      "metadata": {
        "id": "8zLdz7KC0YbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Explanation"
      ],
      "metadata": {
        "id": "jfyAF_SacJZd"
      }
    }
  ]
}